{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":61542,"databundleVersionId":7516023,"sourceType":"competition"},{"sourceId":6868189,"sourceType":"datasetVersion","datasetId":3937441},{"sourceId":6920046,"sourceType":"datasetVersion","datasetId":3973543},{"sourceId":7018354,"sourceType":"datasetVersion","datasetId":4035516},{"sourceId":7426725,"sourceType":"datasetVersion","datasetId":4321427},{"sourceId":7433995,"sourceType":"datasetVersion","datasetId":4326256},{"sourceId":7434193,"sourceType":"datasetVersion","datasetId":4326398}],"dockerImageVersionId":30588,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing libraries, loading and transforming data","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:06:39.370074Z","iopub.execute_input":"2024-01-19T11:06:39.371021Z","iopub.status.idle":"2024-01-19T11:06:39.375010Z","shell.execute_reply.started":"2024-01-19T11:06:39.370985Z","shell.execute_reply":"2024-01-19T11:06:39.373967Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"# The following line uses the 'pip' package manager to install or update several Python packages.\n# - '-U' stands for 'upgrade,' ensuring the packages are updated if they are already installed.\n# - '-q' stands for 'quiet,' which suppresses output messages during installation.\n# - 'mlflow' is being installed or updated, a platform for managing machine learning experiments.\n# - 'datasets' is being installed or updated, a library for easily working with various datasets.\n# - 'nlp' is being installed or updated, which is likely a specific version or extension of the 'datasets' library.\n# - '2>/dev/null' redirects standard error (stderr) to /dev/null, effectively suppressing error messages from the installation process.\n\n# !pip install -U -q mlflow datasets>=2.14.5 nlp 2>/dev/null","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:06:39.382378Z","iopub.execute_input":"2024-01-19T11:06:39.382661Z","iopub.status.idle":"2024-01-19T11:06:39.387579Z","shell.execute_reply.started":"2024-01-19T11:06:39.382636Z","shell.execute_reply":"2024-01-19T11:06:39.386618Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"# !pip install evaluate \n# !pip install /kaggle/input/libraries-tar/evaluate-0.4.1/evaluate-0.4.1 --no-deps --force-reinstall --no-cache-dir\n# !pip install /kaggle/input/libraries-tar/nlp-0.4.0/nlp-0.4.0 --no-build-isolation --no-deps --force-reinstall --no-cache-dir","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:06:39.389545Z","iopub.execute_input":"2024-01-19T11:06:39.389812Z","iopub.status.idle":"2024-01-19T11:06:39.400397Z","shell.execute_reply.started":"2024-01-19T11:06:39.389788Z","shell.execute_reply":"2024-01-19T11:06:39.399576Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"# !mkdir ~/.kaggle\n# !cp /kaggle/input/credentials/kaggle.json ~/.kaggle/\n# !chmod 600 ~/.kaggle/kaggle.json\n\n# import kaggle","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:06:39.401652Z","iopub.execute_input":"2024-01-19T11:06:39.401997Z","iopub.status.idle":"2024-01-19T11:06:39.410866Z","shell.execute_reply.started":"2024-01-19T11:06:39.401973Z","shell.execute_reply":"2024-01-19T11:06:39.410036Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"# %env datasetName = daataset-lib1\n# !mkdir wheels\n# # !kaggle datasets download -d $datasetName -p wheels && unzip wheels/\"$datasetName\".zip -d wheels/\n# !kaggle datasets download -d $datasetName -p wheels && unzip wheels/\"$datasetName\".zip -d wheels/\n# # & unzip wheels/\"$datasetName\".zip -d wheels/","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:06:39.412138Z","iopub.execute_input":"2024-01-19T11:06:39.412478Z","iopub.status.idle":"2024-01-19T11:06:39.420854Z","shell.execute_reply.started":"2024-01-19T11:06:39.412447Z","shell.execute_reply":"2024-01-19T11:06:39.419959Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"!pip install /kaggle/input/daataset-lib1/evaluate-0.4.1-py3-none-any.whl\n!pip install /kaggle/input/daataset-lib1/nlp-0.4.0-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:06:39.423389Z","iopub.execute_input":"2024-01-19T11:06:39.423800Z","iopub.status.idle":"2024-01-19T11:07:44.337400Z","shell.execute_reply.started":"2024-01-19T11:06:39.423768Z","shell.execute_reply":"2024-01-19T11:07:44.336222Z"},"trusted":true},"execution_count":112,"outputs":[{"name":"stdout","text":"Processing /kaggle/input/daataset-lib1/evaluate-0.4.1-py3-none-any.whl\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1) (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1) (1.24.3)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1) (2023.10.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1) (0.17.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate==0.4.1) (11.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate==0.4.1) (3.8.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate==0.4.1) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate==0.4.1) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate==0.4.1) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate==0.4.1) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.1) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.1) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.1) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.1) (2023.7.22)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate==0.4.1) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate==0.4.1) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate==0.4.1) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.1) (1.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate==0.4.1) (1.16.0)\nevaluate is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nProcessing /kaggle/input/daataset-lib1/nlp-0.4.0-py3-none-any.whl\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from nlp==0.4.0) (1.24.3)\nRequirement already satisfied: pyarrow>=0.16.0 in /opt/conda/lib/python3.10/site-packages (from nlp==0.4.0) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from nlp==0.4.0) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from nlp==0.4.0) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from nlp==0.4.0) (2.31.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from nlp==0.4.0) (4.66.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from nlp==0.4.0) (3.12.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from nlp==0.4.0) (3.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->nlp==0.4.0) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->nlp==0.4.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->nlp==0.4.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->nlp==0.4.0) (2023.7.22)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->nlp==0.4.0) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->nlp==0.4.0) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->nlp==0.4.0) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->nlp==0.4.0) (1.16.0)\nnlp is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install /kaggle/input/daataset-pylib/datasets-2.16.1/datasets-2.16.1","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:18:41.825757Z","iopub.execute_input":"2024-01-19T11:18:41.826786Z","iopub.status.idle":"2024-01-19T11:21:32.505850Z","shell.execute_reply.started":"2024-01-19T11:18:41.826740Z","shell.execute_reply":"2024-01-19T11:21:32.504736Z"},"trusted":true},"execution_count":126,"outputs":[{"name":"stdout","text":"Processing /kaggle/input/daataset-pylib/datasets-2.16.1/datasets-2.16.1\n  Installing build dependencies ... \u001b[?25lerror\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m \u001b[31m[8 lines of output]\u001b[0m\n  \u001b[31m   \u001b[0m \u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7c110fc81300>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/setuptools/\u001b[0m\u001b[33m\n  \u001b[31m   \u001b[0m \u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7c110fc81420>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/setuptools/\u001b[0m\u001b[33m\n  \u001b[31m   \u001b[0m \u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7c110fc823e0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/setuptools/\u001b[0m\u001b[33m\n  \u001b[31m   \u001b[0m \u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7c110fc82590>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/setuptools/\u001b[0m\u001b[33m\n  \u001b[31m   \u001b[0m \u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7c110fc82740>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/setuptools/\u001b[0m\u001b[33m\n  \u001b[31m   \u001b[0m \u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement setuptools>=40.8.0 (from versions: none)\u001b[0m\u001b[31m\n  \u001b[31m   \u001b[0m \u001b[0m\u001b[31mERROR: No matching distribution found for setuptools>=40.8.0\u001b[0m\u001b[31m\n  \u001b[31m   \u001b[0m \u001b[0m\n  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n\n\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n\u001b[31m╰─>\u001b[0m See above for output.\n\n\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n\u001b[?25h","output_type":"stream"}]},{"cell_type":"code","source":"# !pip install /kaggle/input/daataset-lib1/tokenizers-0.13.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n# !pip install /kaggle/input/daataset-lib1/transformers-4.30.2-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:07:44.339286Z","iopub.execute_input":"2024-01-19T11:07:44.339613Z","iopub.status.idle":"2024-01-19T11:07:44.344071Z","shell.execute_reply.started":"2024-01-19T11:07:44.339581Z","shell.execute_reply":"2024-01-19T11:07:44.343213Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"# def install_wheels():\n    \n#     # Install `tensorflow-gpu==1.13.1` from pre-downloaded wheels\n#     PATH_TO_TF_WHEELS = '/kaggle/input/daataset-lib1'\n    \n#     # yes, mixing up Python code and bash is ugly. But it's handy \n#     !python -m pip install --no-deps $PATH_TO_TF_WHEELS/*.whl\n    \n# install_wheels()","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:07:44.345578Z","iopub.execute_input":"2024-01-19T11:07:44.345957Z","iopub.status.idle":"2024-01-19T11:07:44.356061Z","shell.execute_reply.started":"2024-01-19T11:07:44.345924Z","shell.execute_reply":"2024-01-19T11:07:44.355230Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"# !pip install --no-build-isolation /kaggle/input/daataset-pylib/evaluate-0.4.1/evaluate-0.4.1","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:07:44.357416Z","iopub.execute_input":"2024-01-19T11:07:44.357712Z","iopub.status.idle":"2024-01-19T11:07:44.372530Z","shell.execute_reply.started":"2024-01-19T11:07:44.357688Z","shell.execute_reply":"2024-01-19T11:07:44.371711Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"#requirements \n# !pip install evaluate\n# !conda install /kaggle/input/daataset-pylib/evaluate-0.4.1/evaluate-0.4.1\n# !cd /kaggle/input/libraries-tar/evaluate-0.4.1/evaluate-0.4.1\n# !pip install evaluate-0.4.1\n# !python setup.py install\n# !cd /kaggle/input/libraries-tar/nlp-0.4.0/nlp-0.4.0/\n# !pip install nlp-0.4.0\n# !python setup.py install","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:07:44.373581Z","iopub.execute_input":"2024-01-19T11:07:44.373875Z","iopub.status.idle":"2024-01-19T11:07:44.383045Z","shell.execute_reply.started":"2024-01-19T11:07:44.373852Z","shell.execute_reply":"2024-01-19T11:07:44.382244Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd  # For data manipulation and analysis\nimport gc  # For garbage collection to manage memory\nimport re  # For regular expressions\nimport numpy as np  # For numerical operations and arrays\n\nimport warnings  # For handling warnings\nwarnings.filterwarnings(\"ignore\")  # Ignore warning messages\n\nimport evaluate\nimport torch  # PyTorch library for deep learning\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig  # Transformers library for natural language processing\nfrom transformers import TextDataset, LineByLineTextDataset, DataCollatorForLanguageModeling, \\\npipeline, Trainer, TrainingArguments, DataCollatorWithPadding  # Transformers components for text processing\nfrom transformers import AutoModelForSequenceClassification  # Transformer model for sequence classification\n\nfrom nlp import Dataset  # Import custom 'Dataset' class for natural language processing tasks\nfrom imblearn.over_sampling import RandomOverSampler  # For oversampling to handle class imbalance\nimport datasets  # Import datasets library\nfrom datasets import Dataset, Image, ClassLabel  # Import custom 'Dataset', 'ClassLabel', and 'Image' classes\nfrom transformers import pipeline  # Transformers library for pipelines\nfrom bs4 import BeautifulSoup  # For parsing HTML content\n\nimport matplotlib.pyplot as plt  # For data visualization\nimport itertools  # For working with iterators\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (  # Import various metrics from scikit-learn\n    accuracy_score,  # For calculating accuracy\n    roc_auc_score,  # For ROC AUC score\n    confusion_matrix,  # For confusion matrix\n    classification_report,  # For classification report\n    f1_score  # For F1 score\n)\n\nfrom datasets import DatasetDict, load_metric  # Import load_metric function to load evaluation metrics\n\nfrom tqdm import tqdm  # For displaying progress bars\ntqdm.pandas()  # Enable progress bars for pandas operations","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:07:44.387934Z","iopub.execute_input":"2024-01-19T11:07:44.388292Z","iopub.status.idle":"2024-01-19T11:07:44.398825Z","shell.execute_reply.started":"2024-01-19T11:07:44.388256Z","shell.execute_reply":"2024-01-19T11:07:44.398001Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"# Set parameters for training a news category classifier using DistilBERT\n\n#lowercase normalization \nLOWERCASE = True\n\n# Fraction of the dataset used for training, the rest will be used for validation\ntrain_fraction = 0.95\n\n# Number of training epochs\nnum_train_epochs = 2\n\n# Learning rate\nlearning_rate = 2e-7\n\n# Batch size for training\ntrain_batch_size = 8\n\n# Batch size for validation\neval_batch_size = 64\n\n# Number of warm-up steps during training\nwarmup_steps = 50\n\n# Weight decay to control regularization during training\nweight_decay = 0.02\n\n# Pre-trained BERT model to be used\nBERT_MODEL = \"distilbert-base-cased\"\n\n# Directory where the model output will be saved\noutput_dir = \"ai-generated-essay-detection-distilbert\"","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:07:44.400036Z","iopub.execute_input":"2024-01-19T11:07:44.400325Z","iopub.status.idle":"2024-01-19T11:07:44.412577Z","shell.execute_reply.started":"2024-01-19T11:07:44.400301Z","shell.execute_reply":"2024-01-19T11:07:44.411741Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"%%time\n# Read the CSV file into a DataFrame\ndf0 = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text-dataset/Training_Essay_Data.csv\", encoding='latin-1')\ndf0 = df0.rename(columns={'generated': 'label', 'text': 'title'}) # Rename the columns to standard ones\nprint(df0.shape, df0.columns)\n\ndf1 = pd.read_csv(\"/kaggle/input/augmented-data-for-llm-detect-ai-generated-text/final_train.csv\")\ndf1 = df1.rename(columns={'text': 'title'}) # Rename the columns to standard ones\nprint(df1.shape, df1.columns)\ndf2 = pd.read_csv(\"/kaggle/input/augmented-data-for-llm-detect-ai-generated-text/final_test.csv\")\ndf2 = df2.rename(columns={'text': 'title'}) # Rename the columns to standard ones\nprint(df2.shape, df2.columns)\ndf3 = pd.read_csv(\"/kaggle/input/llm-generated-essays/ai_generated_train_essays.csv\")\ndf3 = df3.rename(columns={'generated': 'label', 'text': 'title'}) # Rename the columns to standard ones\nprint(df3.shape, df3.columns)\ndf4 = pd.read_csv(\"/kaggle/input/llm-generated-essays/ai_generated_train_essays_gpt-4.csv\")\ndf4 = df4.rename(columns={'generated': 'label', 'text': 'title'}) # Rename the columns to standard ones\nprint(df4.shape, df4.columns)\ndf5 = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/train_essays.csv\")\ndf5 = df5.rename(columns={'generated': 'label', 'text': 'title'}) # Rename the columns to standard ones\nprint(df5.shape, df5.columns)\n\ntest = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\n\ndf = pd.concat([df0, df1, df2, df3, df4, df5], axis=0)\n\nitem0 = df.shape[0]  # Store the initial number of items in the DataFrame\ndf = df.drop_duplicates()  # Remove duplicate rows from the DataFrame\nitem1 = df.shape[0]  # Store the number of items in the DataFrame after removing duplicates\nprint(f\"There are {item0-item1} duplicates found in the dataset\")  # Print the number of duplicates removed\n\ndf = df[['label', 'title']]  # Select only the 'label' and 'title' columns\ndf = df[~df['title'].isnull()]  # Remove rows where 'title' is null\ndf = df[~df['label'].isnull()]  # Remove rows where 'label' is null\n\ndef change_label(x):\n    if x:\n        return 'AI-generated'\n    else:\n        return 'Not AI-generated'\ndf['label'] = df['label'].progress_apply(change_label)\n\n\nprint(df.shape)  # Print the shape of the DataFrame after data preprocessing\ndf.sample(5).T  # Display a random sample of 5 rows from the DataFrame","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:07:44.413975Z","iopub.execute_input":"2024-01-19T11:07:44.414312Z","iopub.status.idle":"2024-01-19T11:08:01.755832Z","shell.execute_reply.started":"2024-01-19T11:07:44.414279Z","shell.execute_reply":"2024-01-19T11:08:01.754881Z"},"trusted":true},"execution_count":119,"outputs":[{"name":"stdout","text":"(29145, 2) Index(['title', 'label'], dtype='object')\n(346977, 2) Index(['title', 'label'], dtype='object')\n(86587, 2) Index(['title', 'label'], dtype='object')\n(500, 4) Index(['id', 'prompt_id', 'title', 'label'], dtype='object')\n(200, 4) Index(['id', 'prompt_id', 'title', 'label'], dtype='object')\n(1378, 4) Index(['id', 'prompt_id', 'title', 'label'], dtype='object')\nThere are 4469 duplicates found in the dataset\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 460318/460318 [00:00<00:00, 691716.77it/s]","output_type":"stream"},{"name":"stdout","text":"(460318, 2)\nCPU times: user 16.2 s, sys: 1.16 s, total: 17.3 s\nWall time: 17.3 s\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"execution_count":119,"output_type":"execute_result","data":{"text/plain":"                                                  79654   \\\nlabel                                       AI-generated   \ntitle  Participating in Adventure\\n\\nGrowing up on a ...   \n\n                                                  39151   \\\nlabel                                   Not AI-generated   \ntitle  Actress and singer Audrey Hepburn once remarke...   \n\n                                                  164355  \\\nlabel                                       AI-generated   \ntitle  Ugh, I know this IQ going to be Quito annoying...   \n\n                                                  315658  \\\nlabel                                   Not AI-generated   \ntitle  Arguing theater use of this technology to read...   \n\n                                                  78983   \nlabel                                       AI-generated  \ntitle  Dear Principal Smith, \\n\\nI am writing to to r...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>79654</th>\n      <th>39151</th>\n      <th>164355</th>\n      <th>315658</th>\n      <th>78983</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>label</th>\n      <td>AI-generated</td>\n      <td>Not AI-generated</td>\n      <td>AI-generated</td>\n      <td>Not AI-generated</td>\n      <td>AI-generated</td>\n    </tr>\n    <tr>\n      <th>title</th>\n      <td>Participating in Adventure\\n\\nGrowing up on a ...</td>\n      <td>Actress and singer Audrey Hepburn once remarke...</td>\n      <td>Ugh, I know this IQ going to be Quito annoying...</td>\n      <td>Arguing theater use of this technology to read...</td>\n      <td>Dear Principal Smith, \\n\\nI am writing to to r...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"**In a machine learning context, class weights are used to address the issue of class imbalance in a dataset. Class imbalance occurs when one class (or several classes) in a classification problem has significantly more or fewer instances than others. This can lead to models being biased towards the majority class and performing poorly on the minority class(es).**\n\n**Class weights assign different weights to classes during model training to give more importance to underrepresented classes. They help the model to pay more attention to minority classes, thereby improving its ability to make accurate predictions across all classes.**\n\n**These weights are used by algorithms (like logistic regression, SVM, decision trees, etc.) during training to adjust the importance of different classes in the overall loss function, ensuring that the model doesn't prioritize accuracy on the majority class at the expense of the minority classes.**","metadata":{}},{"cell_type":"code","source":"# Import the necessary library to compute class weights.\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# Identify the unique classes in the training data.\nclasses = ['AI-generated', 'Not AI-generated'] # np.unique(df[['label']])\n\nprint(classes)\n\n# Calculate class weights using the 'balanced' option, which automatically adjusts for class imbalance.\nweights = compute_class_weight(class_weight='balanced', classes=classes, y=df['label'])\n\n# Create a dictionary mapping each class to its respective class weight.\nclass_weights = dict(zip(classes, weights))\n\n# Print the computed class weights to the console.\nprint(class_weights)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:01.757320Z","iopub.execute_input":"2024-01-19T11:08:01.757628Z","iopub.status.idle":"2024-01-19T11:08:01.934837Z","shell.execute_reply.started":"2024-01-19T11:08:01.757602Z","shell.execute_reply":"2024-01-19T11:08:01.933876Z"},"trusted":true},"execution_count":120,"outputs":[{"name":"stdout","text":"['AI-generated', 'Not AI-generated']\n{'AI-generated': 1.3958759135154806, 'Not AI-generated': 0.7790565035050249}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create a list of unique labels\nlabels_list = classes\n\n# Initialize empty dictionaries to map labels to IDs and vice versa\nlabel2id, id2label = dict(), dict()\n\n# Iterate over the unique labels and assign each label an ID, and vice versa\nfor i, label in enumerate(labels_list):\n    label2id[label] = i  # Map the label to its corresponding ID\n    id2label[i] = label  # Map the ID to its corresponding label\n\n# Print the resulting dictionaries for reference\nprint(\"Mapping of IDs to Labels:\", id2label, '\\n')\nprint(\"Mapping of Labels to IDs:\", label2id)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:01.936294Z","iopub.execute_input":"2024-01-19T11:08:01.936594Z","iopub.status.idle":"2024-01-19T11:08:01.943472Z","shell.execute_reply.started":"2024-01-19T11:08:01.936569Z","shell.execute_reply":"2024-01-19T11:08:01.942481Z"},"trusted":true},"execution_count":121,"outputs":[{"name":"stdout","text":"Mapping of IDs to Labels: {0: 'AI-generated', 1: 'Not AI-generated'} \n\nMapping of Labels to IDs: {'AI-generated': 0, 'Not AI-generated': 1}\n","output_type":"stream"}]},{"cell_type":"code","source":"ordered_weigths = [class_weights[x] for x in id2label.values()]\nordered_weigths","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:01.944954Z","iopub.execute_input":"2024-01-19T11:08:01.945228Z","iopub.status.idle":"2024-01-19T11:08:01.960389Z","shell.execute_reply.started":"2024-01-19T11:08:01.945181Z","shell.execute_reply":"2024-01-19T11:08:01.959511Z"},"trusted":true},"execution_count":122,"outputs":[{"execution_count":122,"output_type":"execute_result","data":{"text/plain":"[1.3958759135154806, 0.7790565035050249]"},"metadata":{}}]},{"cell_type":"code","source":"# Create a dataset from the Pandas DataFrame 'df'\ndataset = Dataset.from_pandas(df)\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:01.961663Z","iopub.execute_input":"2024-01-19T11:08:01.962003Z","iopub.status.idle":"2024-01-19T11:08:03.709104Z","shell.execute_reply.started":"2024-01-19T11:08:01.961978Z","shell.execute_reply":"2024-01-19T11:08:03.707760Z"},"trusted":true},"execution_count":123,"outputs":[{"execution_count":123,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['label', 'title', '__index_level_0__'],\n    num_rows: 460318\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Creating classlabels to match labels to IDs\nClassLabels = ClassLabel(num_classes=len(labels_list), names=labels_list)\n\n# Mapping labels to IDs\ndef map_label2id(example):\n    example['label'] = ClassLabels.str2int(example['label'])\n    return example\n\ndataset = dataset.map(map_label2id, batched=True)\n\n# Casting label column to ClassLabel Object\ndataset = dataset.cast_column('label', ClassLabels)\n\n# Splitting the dataset into training and testing sets using the predefined train/test split ratio.\ndataset = dataset.train_test_split(test_size=1-train_fraction, shuffle=True, stratify_by_column=\"label\")\n\n# Extracting the training data from the split dataset.\ndf_train = dataset['train']\n\n# Extracting the testing data from the split dataset.\ndf_test = dataset['test']","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:03.711545Z","iopub.execute_input":"2024-01-19T11:08:03.712494Z","iopub.status.idle":"2024-01-19T11:08:13.019649Z","shell.execute_reply.started":"2024-01-19T11:08:03.712454Z","shell.execute_reply":"2024-01-19T11:08:13.015655Z"},"trusted":true},"execution_count":124,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/461 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ec346f9f72049ebb86ca15e793701cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Casting the dataset:   0%|          | 0/47 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bd3de7c23bb442da6fac249c6255d65"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[124], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mcast_column(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, ClassLabels)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Splitting the dataset into training and testing sets using the predefined train/test split ratio.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mtrain_fraction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify_by_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Extracting the training data from the split dataset.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m df_train \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:487\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    485\u001b[0m }\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 487\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    489\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/fingerprint.py:458\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m             kwargs[fingerprint_name] \u001b[38;5;241m=\u001b[39m update_fingerprint(\n\u001b[1;32m    453\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fingerprint, transform, kwargs_for_fingerprint\n\u001b[1;32m    454\u001b[0m             )\n\u001b[1;32m    456\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 458\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n","\u001b[0;31mTypeError\u001b[0m: Dataset.train_test_split() got an unexpected keyword argument 'stratify_by_column'"],"ename":"TypeError","evalue":"Dataset.train_test_split() got an unexpected keyword argument 'stratify_by_column'","output_type":"error"}]},{"cell_type":"code","source":"# Deleting the DataFrame 'df'\ndel df\n\n# Performing garbage collection to free up memory\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:13.021061Z","iopub.status.idle":"2024-01-19T11:08:13.021472Z","shell.execute_reply.started":"2024-01-19T11:08:13.021279Z","shell.execute_reply":"2024-01-19T11:08:13.021298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**** NFC and NFD are normalization forms used in Unicode text encoding.***\n\n**NFC (Normalization Form Canonical Composition): In NFC normalization, a text is transformed into a composed form where characters are replaced with their composed forms whenever possible. This means that if a character can be represented as a single Unicode code point or a combination of code points (composed character sequence), NFC will convert it to the single code point when possible.**\n\n**NFD (Normalization Form Canonical Decomposition): NFD normalization decomposes characters into their canonical decomposed forms. It replaces characters with their decomposed forms, consisting of a base character followed by combining characters where applicable. This form breaks down characters into their basic components.**\n\n**These normalization forms are important in text processing, especially when dealing with multilingual text, as they help ensure consistency in representations. NFC and NFD normalization help to reduce different representations of the same text, enabling better text comparison, indexing, and handling across various systems and languages.**","metadata":{}},{"cell_type":"code","source":"from tokenizers import normalizers\nfrom tokenizers.normalizers import StripAccents, Lowercase, NFC\nfrom tokenizers import NormalizedString\n\nnormalizer = normalizers.Sequence([NFC(), StripAccents()] + [Lowercase()] if LOWERCASE else [])\n\ndef preprocess_function(examples):\n    # Normalize each title separately\n    normalized_titles = [normalizer.normalize(NormalizedString(title)) for title in examples['title']]\n    \n    # Convert NormalizedString objects back to strings\n    normalized_titles = [str(title) for title in normalized_titles]\n    \n    # Tokenize the normalized titles\n    tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL, use_fast=True, low_cpu_mem_usage=False)\n    examples = tokenizer(normalized_titles, max_length=512, truncation=True)\n    \n    return examples\n\n# Apply the preprocess_function to your dataset\ndf_train = df_train.map(preprocess_function, batched=True)\ndf_test = df_test.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:13.025257Z","iopub.status.idle":"2024-01-19T11:08:13.026121Z","shell.execute_reply.started":"2024-01-19T11:08:13.025836Z","shell.execute_reply":"2024-01-19T11:08:13.025864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL, use_fast=True, low_cpu_mem_usage=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:13.027291Z","iopub.status.idle":"2024-01-19T11:08:13.027650Z","shell.execute_reply.started":"2024-01-19T11:08:13.027477Z","shell.execute_reply":"2024-01-19T11:08:13.027494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tokenizers import normalizers\nfrom tokenizers.normalizers import NFD, StripAccents, Lowercase, NFC\n\n#check for NFC and NFD normalization\nnormalizer = normalizers.Sequence([NFC(), StripAccents()] + [Lowercase()] if LOWERCASE else [])\nspecial_tokens = {\n    \"unk_token\": \"[UNK]\",    # Unknown token for out-of-vocabulary words\n    \"cls_token\": \"[CLS]\",    # Classification token\n    \"pad_token\": \"[PAD]\",    # Padding token\n    \"sep_token\": \"[SEP]\",    # Separation token\n    \"mask_token\": \"[MASK]\"   # Mask token\n}\nmax_len = 512\n\n# Create a tokenizer instance for the specified BERT model.\n# - 'AutoTokenizer.from_pretrained' loads the pre-trained tokenizer for the specified model.\n# - 'use_fast=True' enables fast tokenization, which is recommended for most use cases.\n# - 'low_cpu_mem_usage=False' disables low CPU memory usage mode (useful for larger models).\ntokenizer = AutoTokenizer.from_pretrained(BERT_MODEL, use_fast=True, low_cpu_mem_usage=False)\n# tokenizer.normalizer = normalizer\n# tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL, use_fast=True, low_cpu_mem_usage=False, normalizer=normalizer)\n# tokenizer.add_special_tokens(special_tokens)\n# tokenizer.save_pretrained(\"path/to/save\")","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:13.028930Z","iopub.status.idle":"2024-01-19T11:08:13.029353Z","shell.execute_reply.started":"2024-01-19T11:08:13.029123Z","shell.execute_reply":"2024-01-19T11:08:13.029146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# type(df_train['title'])","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:13.031305Z","iopub.status.idle":"2024-01-19T11:08:13.031644Z","shell.execute_reply.started":"2024-01-19T11:08:13.031477Z","shell.execute_reply":"2024-01-19T11:08:13.031493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importantly, this is a simple function for preprocessing data before training a natural language processing model.\n# It takes a dataset of examples as input.\n\ndef preprocess_function(examples):\n    # The main task of this function is to tokenize the text data in the 'title' column of the examples.\n    # Tokenization is the process of breaking down text into smaller units, such as words or subwords.\n    # In this case, the tokenizer is applied to each 'title' in the examples.\n#     .... CHANGE -- .... \n#     tokenizer.truncation_side = \"left\"\n    # The 'truncation=True' parameter indicates that if a title is too long to fit within the model's maximum input length,\n    # it should be truncated to fit. Truncation can help ensure that the input data is within the model's capacity.\n    \n    normalized_titles = [normalizer.normalize(examples[title]) for title in examples['title']]\n    examples = tokenizer(normalized_text['title'], max_length=512, truncation=True)\n    return examples\n\n#.... CHANGE -- .... \n# def preprocess_function(examples):\n#     # Tokenize each essay in the \"title\" column separately, without truncation\n#     encoded_chunks = tokenizer.batch_encode_plus(\n#         examples[\"title\"],\n#         max_length=max_len,\n#         truncation=False,\n#         padding='max_length',\n#         return_attention_mask=True,\n#         return_tensors='pt'\n#     )\n\n#     return {\n#         'input_ids': encoded_chunks['input_ids'],\n#         'attention_mask': encoded_chunks['attention_mask']\n#     }\n#     return tokenizer(examples[\"title\"], truncation=True, max_length = 512, return_tensors=\"np\")\n#     return tokenizer(examples[\"title\"], truncation=True)\n\n\n# The code below applies the preprocess_function to two dataframes, df_train and df_test.\n\n# df_train is likely a training dataset, and df_test is likely a testing dataset.\n# These datasets contain examples with a 'title' field that we want to tokenize for further processing.\n\n# The 'map' function is used to apply the preprocess_function to each example in the datasets.\n# The 'batched=True' parameter indicates that the tokenization should be applied in batches for efficiency.\n\ndf_train = df_train.map(preprocess_function, batched=True)\ndf_test = df_test.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:13.032868Z","iopub.status.idle":"2024-01-19T11:08:13.033259Z","shell.execute_reply.started":"2024-01-19T11:08:13.033056Z","shell.execute_reply":"2024-01-19T11:08:13.033072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove the 'title' column from the training dataset.\ndf_train = df_train.remove_columns(['title'])\n\n# Remove the 'title' column from the testing dataset.\ndf_test = df_test.remove_columns(['title'])","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:13.035603Z","iopub.status.idle":"2024-01-19T11:08:13.035939Z","shell.execute_reply.started":"2024-01-19T11:08:13.035773Z","shell.execute_reply":"2024-01-19T11:08:13.035789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:13.036803Z","iopub.status.idle":"2024-01-19T11:08:13.037163Z","shell.execute_reply.started":"2024-01-19T11:08:13.036987Z","shell.execute_reply":"2024-01-19T11:08:13.037004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:13.038965Z","iopub.status.idle":"2024-01-19T11:08:13.039392Z","shell.execute_reply.started":"2024-01-19T11:08:13.039149Z","shell.execute_reply":"2024-01-19T11:08:13.039165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DataCollatorWithPadding creates batch of data. It also dynamically pads text to the \n#  length of the longest element in the batch, making them all the same length. \n#  It's possible to pad your text in the tokenizer function with padding=True, dynamic padding is more efficient.\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:13.040604Z","iopub.status.idle":"2024-01-19T11:08:13.040966Z","shell.execute_reply.started":"2024-01-19T11:08:13.040785Z","shell.execute_reply":"2024-01-19T11:08:13.040802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nDynamic padding refers to a padding strategy where each batch of input sequences is padded based on the length of the longest sequence in that specific batch. This is in contrast to static padding, where all sequences are padded to a fixed maximum length determined by the longest sequence in the entire dataset.\n\nDynamic padding can be particularly useful in scenarios where the lengths of sequences vary significantly within the dataset. By dynamically adjusting the padding length for each batch, you can reduce unnecessary padding for shorter sequences, optimizing memory usage and computational efficiency during training.\n\nIn the context of natural language processing (NLP) and neural network training, dynamic padding is often implemented in the DataCollator or data preprocessing stage. When using dynamic padding, the padding length for each batch is determined by the maximum sequence length in that specific batch.","metadata":{}},{"cell_type":"code","source":"# Retrieve the 'input_ids' from the first row of the DataFrame 'df_train'\ntokenizer.decode(df_train[0]['input_ids'])","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:13.042869Z","iopub.status.idle":"2024-01-19T11:08:13.043266Z","shell.execute_reply.started":"2024-01-19T11:08:13.043060Z","shell.execute_reply":"2024-01-19T11:08:13.043077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading and training model","metadata":{}},{"cell_type":"code","source":"len(labels_list)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:13.044489Z","iopub.status.idle":"2024-01-19T11:08:13.044851Z","shell.execute_reply.started":"2024-01-19T11:08:13.044674Z","shell.execute_reply":"2024-01-19T11:08:13.044691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load a pre-trained BERT-based model for sequence classification.\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    BERT_MODEL, num_labels=len(labels_list),\n    output_attentions=False,  # Set to False: Model will not return attention weights.\n    output_hidden_states=False  # Set to False: Model will not return all hidden-states.\n)\n\n# Configure the mapping of class labels to their corresponding indices for later reference.\nmodel.config.id2label = id2label  # Mapping from label indices to class labels.\nmodel.config.label2id = label2id  # Mapping from class labels to label indices.\n\n# Calculate and print the number of trainable parameters in millions for the model.\nprint(model.num_parameters(only_trainable=True) / 1e6)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:13.046123Z","iopub.status.idle":"2024-01-19T11:08:13.046543Z","shell.execute_reply.started":"2024-01-19T11:08:13.046356Z","shell.execute_reply":"2024-01-19T11:08:13.046375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the 'load_metric' function from the Hugging Face datasets library to load a metric.\nmetric = load_metric(\"accuracy\")\n\n# Define a custom 'compute_metrics' function that will be used for evaluating model performance.\n# This function takes 'eval_pred' as input, which is a tuple containing predicted logits and true labels.\ndef compute_metrics(eval_pred):\n    # Unpack the 'eval_pred' tuple into 'logits' (predicted logits) and 'labels' (true labels).\n    logits, labels = eval_pred\n    \n    # Calculate the model's predictions by selecting the class with the highest logit value.\n    predictions = np.argmax(logits, axis=-1)\n    \n    # Use the imported metric to compute the accuracy of the model's predictions.\n    accuracy = metric.compute(predictions=predictions, references=labels)\n    \n    # Return the computed accuracy as the evaluation metric.\n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:13.048106Z","iopub.status.idle":"2024-01-19T11:08:13.048493Z","shell.execute_reply.started":"2024-01-19T11:08:13.048315Z","shell.execute_reply":"2024-01-19T11:08:13.048333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WeightedTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        # forward pass\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n        # compute custom loss (suppose one has labels with different weights)\n        loss_fct = torch.nn.CrossEntropyLoss(weight=torch.tensor(ordered_weigths, device = logits.device).float())\n        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n        return (loss, outputs) if return_outputs else loss","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:13.049856Z","iopub.status.idle":"2024-01-19T11:08:13.050256Z","shell.execute_reply.started":"2024-01-19T11:08:13.050045Z","shell.execute_reply":"2024-01-19T11:08:13.050062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create TrainingArguments to configure the training process\ntraining_args = TrainingArguments(\n    output_dir=output_dir,  # Directory to save the model checkpoints and logs\n    logging_dir='./logs',  # Directory to store training logs\n    num_train_epochs=num_train_epochs,  # Number of training epochs\n    per_device_train_batch_size=train_batch_size,  # Batch size for training data\n    per_device_eval_batch_size=eval_batch_size,  # Batch size for evaluation data\n    logging_strategy='steps',  # Logging frequency during training (steps or epoch)\n    logging_first_step=True,  # Log the first training step\n    load_best_model_at_end=True,  # Load the best model at the end of training\n    logging_steps=1,  # Log every training step (useful for debugging)\n    learning_rate=learning_rate, # Set the learning rate for the optimizer.\n    evaluation_strategy='epoch',  # Evaluation frequency (epoch or steps)\n    warmup_steps=warmup_steps,  # Number of warmup steps for the learning rate\n    weight_decay=weight_decay,  # Weight decay for regularization\n    eval_steps=1,  # Evaluate every training step (useful for debugging)\n    save_strategy='epoch',  # Save model checkpoints every epoch\n    save_total_limit=1,  # Limit the number of saved checkpoints to save space\n    report_to=\"mlflow\",  # Log training metrics to MLflow\n)\n\n# Define the trainer:\n# Instantiate the trainer class and configure its settings\ntrainer = WeightedTrainer(\n    model=model,  # The pretrained or custom model to be trained\n    args=training_args, \n    # TrainingArguments for configuring training\n    compute_metrics=compute_metrics,  # Function for computing evaluation metrics\n    train_dataset=df_train,  # Training dataset\n    eval_dataset=df_test,  # Evaluation dataset\n    data_collator=data_collator  # Data collator for batching and preprocessing\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:13.051766Z","iopub.status.idle":"2024-01-19T11:08:13.052133Z","shell.execute_reply.started":"2024-01-19T11:08:13.051957Z","shell.execute_reply":"2024-01-19T11:08:13.051974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get initial metrics\ntrainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:13.053308Z","iopub.status.idle":"2024-01-19T11:08:13.053671Z","shell.execute_reply.started":"2024-01-19T11:08:13.053495Z","shell.execute_reply":"2024-01-19T11:08:13.053512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Start training the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:13.054965Z","iopub.status.idle":"2024-01-19T11:08:13.055338Z","shell.execute_reply.started":"2024-01-19T11:08:13.055133Z","shell.execute_reply":"2024-01-19T11:08:13.055148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final model evaluation\ntrainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:13.056498Z","iopub.status.idle":"2024-01-19T11:08:13.056826Z","shell.execute_reply.started":"2024-01-19T11:08:13.056663Z","shell.execute_reply":"2024-01-19T11:08:13.056678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:13.058560Z","iopub.status.idle":"2024-01-19T11:08:13.058918Z","shell.execute_reply.started":"2024-01-19T11:08:13.058743Z","shell.execute_reply":"2024-01-19T11:08:13.058760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use the trained 'trainer' to make predictions on the 'df_test'.\noutputs = trainer.predict(df_test)\n# Print the metrics obtained from the prediction outputs.\nprint(outputs.metrics)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:13.060600Z","iopub.status.idle":"2024-01-19T11:08:13.060932Z","shell.execute_reply.started":"2024-01-19T11:08:13.060767Z","shell.execute_reply":"2024-01-19T11:08:13.060783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# import torch.nn.functional as F\n# import numpy as np\n\n# # Assuming outputs.logits is a tensor with shape (batch_size, num_classes)\n# logits = torch.tensor(outputs.predictions)\n\n# # Apply softmax to get probabilities\n# probabilities = F.softmax(logits, dim=1)\n\n# # Convert the probabilities tensor to a numpy array\n# probabilities_np = probabilities.cpu().numpy()\n\n# # Access probabilities for each class\n# class_0_probabilities = probabilities_np[:, 0]\n# class_1_probabilities = probabilities_np[:, 1]\n\n# # Print or use the probabilities as needed\n# print(\"Probabilities for Class 0:\", class_0_probabilities)\n# print(\"Probabilities for Class 1:\", class_1_probabilities)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:13.062598Z","iopub.status.idle":"2024-01-19T11:08:13.063019Z","shell.execute_reply.started":"2024-01-19T11:08:13.062828Z","shell.execute_reply":"2024-01-19T11:08:13.062846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract the true labels from the model outputs\ny_true = outputs.label_ids\n\n# Predict the labels by selecting the class with the highest probability\ny_pred = outputs.predictions.argmax(1)\n\n# Define a function to plot a confusion matrix\ndef plot_confusion_matrix(cm, classes, title='Confusion Matrix', cmap=plt.cm.Blues, figsize=(10, 8), is_norm=True):\n    \"\"\"\n    This function plots a confusion matrix.\n\n    Parameters:\n        cm (array-like): Confusion matrix as returned by sklearn.metrics.confusion_matrix.\n        classes (list): List of class names, e.g., ['Class 0', 'Class 1'].\n        title (str): Title for the plot.\n        cmap (matplotlib colormap): Colormap for the plot.\n    \"\"\"\n    # Create a figure with a specified size\n    plt.figure(figsize=figsize)\n    \n    \n    # Display the confusion matrix as an image with a colormap\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    # Define tick marks and labels for the classes on the axes\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n    \n    if is_norm:\n        fmt = '.3f'\n    else:\n        fmt = '.0f'\n    # Add text annotations to the plot indicating the values in the cells\n    thresh = cm.max() / 2.0\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    # Label the axes\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n    # Ensure the plot layout is tight\n    plt.tight_layout()\n    # Display the plot\n    plt.show()\n\n# Calculate accuracy and F1 score\naccuracy = accuracy_score(y_true, y_pred)\nf1 = f1_score(y_true, y_pred, average='macro')\n\n# Display accuracy and F1 score\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\n\n# Get the confusion matrix if there are a relatively small number of labels\nif len(labels_list) <= 120:\n    # Compute the confusion matrix\n    cm = confusion_matrix(y_true, y_pred, normalize='true')\n\n    # Plot the confusion matrix using the defined function\n    plot_confusion_matrix(cm, labels_list, figsize=(8, 6))\n\n# Finally, display classification report\nprint()\nprint(\"Classification report:\")\nprint()\nprint(classification_report(y_true, y_pred, target_names=labels_list, digits=4))","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:13.064508Z","iopub.status.idle":"2024-01-19T11:08:13.064863Z","shell.execute_reply.started":"2024-01-19T11:08:13.064689Z","shell.execute_reply":"2024-01-19T11:08:13.064706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saving the model and checking its performance with a sample input","metadata":{}},{"cell_type":"code","source":"trainer.save_model()","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:13.066139Z","iopub.status.idle":"2024-01-19T11:08:13.066526Z","shell.execute_reply.started":"2024-01-19T11:08:13.066349Z","shell.execute_reply":"2024-01-19T11:08:13.066366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.save_vocabulary(save_directory=f\"./{output_dir}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:13.069068Z","iopub.status.idle":"2024-01-19T11:08:13.069612Z","shell.execute_reply.started":"2024-01-19T11:08:13.069341Z","shell.execute_reply":"2024-01-19T11:08:13.069366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a classification pipeline and test with the sample input\npipe = pipeline(\"text-classification\", output_dir, tokenizer=BERT_MODEL)\nsample_title = '''Elon Musk buys Twitter'''\npipe(sample_title, top_k=10)\npipe(sample_title, top_k=10)[0]['score']","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:13.071790Z","iopub.status.idle":"2024-01-19T11:08:13.072329Z","shell.execute_reply.started":"2024-01-19T11:08:13.072043Z","shell.execute_reply":"2024-01-19T11:08:13.072068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv')\ntest = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:13.073610Z","iopub.status.idle":"2024-01-19T11:08:13.074088Z","shell.execute_reply.started":"2024-01-19T11:08:13.073844Z","shell.execute_reply":"2024-01-19T11:08:13.073867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_preds = [pipe(sample_title, top_k=10)[0]['score'] for sample_title in test['text'].tolist()]\nsub['generated'] = pd.Series(final_preds)\nsub.to_csv('submission.csv', index=False)\nsub","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:13.075740Z","iopub.status.idle":"2024-01-19T11:08:13.076334Z","shell.execute_reply.started":"2024-01-19T11:08:13.076008Z","shell.execute_reply":"2024-01-19T11:08:13.076037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Send model to Huggingface","metadata":{}},{"cell_type":"code","source":"# # finally, save the model to Huggingface\n# from huggingface_hub import notebook_login\n# notebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:13.077990Z","iopub.status.idle":"2024-01-19T11:08:13.078485Z","shell.execute_reply.started":"2024-01-19T11:08:13.078236Z","shell.execute_reply":"2024-01-19T11:08:13.078259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from huggingface_hub import HfApi\n# api = HfApi()\n# repo_id = f\"yashsshrivastava/{output_dir}\"\n# try:\n#     api.create_repo(repo_id)\n#     print(f\"Repo {repo_id} created\")\n# except:\n#     print(f\"Repo {repo_id} already exists\")\n#     api.delete_repo(repo_id, repo_type= \"model\")\n#     api.create_repo(repo_id)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:13.080441Z","iopub.status.idle":"2024-01-19T11:08:13.080924Z","shell.execute_reply.started":"2024-01-19T11:08:13.080667Z","shell.execute_reply":"2024-01-19T11:08:13.080690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# api.upload_folder(\n#     folder_path=output_dir,\n#     path_in_repo = \".\",\n#     repo_id=repo_id,\n#     repo_type=\"model\",\n#     revision=\"main\" # Revision name\n# )","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:13.082661Z","iopub.status.idle":"2024-01-19T11:08:13.083147Z","shell.execute_reply.started":"2024-01-19T11:08:13.082897Z","shell.execute_reply":"2024-01-19T11:08:13.082920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}